{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W0s09-1yseh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "945e6ebe-f356-47de-d771-6139a7f9c058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Owlready2\n",
            "  Downloading Owlready2-0.41.tar.gz (27.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.4/27.4 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: Owlready2\n",
            "  Building wheel for Owlready2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Owlready2: filename=Owlready2-0.41-cp39-cp39-linux_x86_64.whl size=24160898 sha256=c70ec0ef612f8605190f5dacaa72b173f90423fedf2c94a90ab4d5a825c2158d\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/56/67/10bd2f89fc6262317ee02f4d8dbdff95f1db434b6bb18daed0\n",
            "Successfully built Owlready2\n",
            "Installing collected packages: Owlready2\n",
            "Successfully installed Owlready2-0.41\n"
          ]
        }
      ],
      "source": [
        "!pip install Owlready2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKQdpo4kPPbX",
        "outputId": "f5e40e6b-88bd-456f-a0b0-41e61a0a0853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ngram\n",
            "  Downloading ngram-4.0.3-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: ngram\n",
            "Successfully installed ngram-4.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install ngram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Wei5eZTSITk",
        "outputId": "b1b1b9cd-c642-4184-ac04-e7f48806ea76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting py-stringmatching\n",
            "  Downloading py_stringmatching-0.4.3.tar.gz (643 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/643.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m643.8/643.8 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from py-stringmatching) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from py-stringmatching) (1.16.0)\n",
            "Building wheels for collected packages: py-stringmatching\n",
            "  Building wheel for py-stringmatching (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-stringmatching: filename=py_stringmatching-0.4.3-cp39-cp39-linux_x86_64.whl size=2601552 sha256=8048a7c5d50c1e3d80db4f690c95e0032865e7a422f4a2bdd76d888c2d6096ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/24/f9/e2/89e3ea9801245b19c9c6c365eb4c25afde526cea5e1e296ad9\n",
            "Successfully built py-stringmatching\n",
            "Installing collected packages: py-stringmatching\n",
            "Successfully installed py-stringmatching-0.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install py-stringmatching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xfvNKt0XxLXv"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "from re import finditer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from owlready2 import get_ontology"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifwlzEwgfbnG",
        "outputId": "906f59bd-ae68-4066-ef28-46ad93cf9829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "47_M-OeNZNKc"
      },
      "outputs": [],
      "source": [
        "import ngram\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.corpus import wordnet\n",
        "from py_stringmatching.similarity_measure.affine import Affine\n",
        "from py_stringmatching.similarity_measure.hamming_distance import HammingDistance\n",
        "from py_stringmatching.similarity_measure.cosine import Cosine\n",
        "from py_stringmatching.similarity_measure.dice import Dice\n",
        "from py_stringmatching.similarity_measure.jaccard import Jaccard\n",
        "from py_stringmatching.similarity_measure.jaro import Jaro\n",
        "from py_stringmatching.similarity_measure.jaro_winkler import JaroWinkler\n",
        "from py_stringmatching.similarity_measure.levenshtein import Levenshtein\n",
        "from py_stringmatching.similarity_measure.monge_elkan import MongeElkan\n",
        "from py_stringmatching.similarity_measure.needleman_wunsch import NeedlemanWunsch\n",
        "from py_stringmatching.similarity_measure.overlap_coefficient import OverlapCoefficient\n",
        "from py_stringmatching.similarity_measure.smith_waterman import SmithWaterman\n",
        "from py_stringmatching.similarity_measure.soft_tfidf import SoftTfIdf\n",
        "from py_stringmatching.similarity_measure.tfidf import TfIdf\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-BTuUUeYMdq",
        "outputId": "06db6d7b-4048-45b9-d88a-6a34a3db3370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkV1rRLP2L1i"
      },
      "source": [
        "# Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ShFUEwVnxwXf"
      },
      "outputs": [],
      "source": [
        "def read_ontology(path):\n",
        "    onto = get_ontology(path)\n",
        "    onto.load()\n",
        "\n",
        "    # read classes\n",
        "    classes = []\n",
        "    for cls in onto.classes():\n",
        "        classes.append(cls)\n",
        "    # print(classes)\n",
        "    # print(len(classes))\n",
        "    # print(len(set(classes)))\n",
        "    classes = list(set(classes))  # removing any number of repetitions of a class\n",
        "\n",
        "    # read properties\n",
        "    properties = []\n",
        "    for prop in onto.properties():\n",
        "        properties.append(prop)\n",
        "    properties = list(set(properties))\n",
        "\n",
        "    return classes, properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5xhCMqYBzrEg"
      },
      "outputs": [],
      "source": [
        "def get_mappings(path):\n",
        "    mappings = []\n",
        "    with open(path) as map_file:\n",
        "        soup = BeautifulSoup(map_file, 'xml')\n",
        "\n",
        "    for cell in soup.find_all('Cell'):\n",
        "        entity1 = cell.find('entity1')['rdf:resource'].split('#')[1]\n",
        "        entity2 = cell.find('entity2')['rdf:resource'].split('#')[1]\n",
        "        mapping = (entity1, entity2)\n",
        "        mappings.append(mapping)\n",
        "\n",
        "    return mappings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "p2KwvdjI4Jvk"
      },
      "outputs": [],
      "source": [
        "def get_path(cls):\n",
        "    path = cls.name\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            path = path + '/' + cls.is_a[0].name\n",
        "        except:\n",
        "            break\n",
        "        cls = cls.is_a[0]\n",
        "        if cls == 'owl.Thing':\n",
        "            break\n",
        "    \n",
        "    return '/'.join(path.split('/')[::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Vwzm5ril9Ha-"
      },
      "outputs": [],
      "source": [
        "def get_dataset(onto1, onto2, map_file):\n",
        "    data = []\n",
        "\n",
        "    mappings = get_mappings(map_file)\n",
        "    classes1, properties1 = read_ontology(onto1)\n",
        "    classes2, properties2 = read_ontology(onto2)\n",
        "\n",
        "    class_pairs = list(itertools.product(classes1, classes2))\n",
        "    for pair in class_pairs:\n",
        "        temp = pair\n",
        "        pair = (pair[0].name, pair[1].name)\n",
        "        if pair in mappings:\n",
        "            matched = 1\n",
        "        else:\n",
        "            matched = 0\n",
        "\n",
        "        data.append((onto1, onto2, pair[0], pair[1], temp[0], temp[1], get_path(temp[0]), get_path(temp[1]), matched, 'Class'))\n",
        "\n",
        "    property_pairs = list(itertools.product(properties1, properties2))\n",
        "    for pair in property_pairs:\n",
        "        temp = pair\n",
        "        pair = (pair[0].name, pair[1].name)\n",
        "        if pair in mappings:\n",
        "            matched = 1\n",
        "        else:\n",
        "            matched = 0\n",
        "\n",
        "        data.append((onto1, onto2, pair[0], pair[1], temp[0], temp[1], get_path(temp[0]), get_path(temp[1]), matched, 'Property'))\n",
        "\n",
        "    df = pd.DataFrame(data, columns=['Ontology1', 'Ontology2', 'Entity1', 'Entity2', 'Parent1', 'Parent2', 'Full Path1', 'Full Path2', 'Match', 'Type'])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0lpumerS1KQI"
      },
      "outputs": [],
      "source": [
        "ontologies = ['101.rdf', '102.rdf', '103.rdf', '104.rdf', '204.rdf', '205.rdf', '225.rdf', '230.rdf', '301.rdf', '302.rdf', '303.rdf', '304.rdf']\n",
        "train_aligns = ['101-101.rdf', '101-102.rdf', '101-103.rdf', '101-104.rdf']\n",
        "test_aligns = ['101-225.rdf', '101-230.rdf', '101-204.rdf', '101-205.rdf', '101-301.rdf', '101-302.rdf', '101-303.rdf', '101-304.rdf']\n",
        "\n",
        "train_datasets = []\n",
        "\n",
        "# Create dataset\n",
        "for align in train_aligns:\n",
        "    ont1, ont2 = align.split('.')[0].split('-')\n",
        "\n",
        "    ont1_path = '/content/drive/MyDrive/Dataset/ontologies/' + ont1 + '.rdf'\n",
        "    ont2_path = '/content/drive/MyDrive/Dataset/ontologies/' + ont2 + '.rdf'\n",
        "    alignment_path = '/content/drive/MyDrive/Dataset/alignments/' + align\n",
        "\n",
        "    train_datasets.append(get_dataset(ont1_path, ont2_path, alignment_path))\n",
        "\n",
        "train = pd.concat(train_datasets, ignore_index=True)\n",
        "\n",
        "test_datasets = []\n",
        "\n",
        "for align in test_aligns:\n",
        "    ont1, ont2 = align.split('.')[0].split('-')\n",
        "\n",
        "    ont1_path = '/content/drive/MyDrive/Dataset/ontologies/' + ont1 + '.rdf'\n",
        "    ont2_path = '/content/drive/MyDrive/Dataset/ontologies/' + ont2 + '.rdf'\n",
        "    alignment_path = '/content/drive/MyDrive/Dataset/alignments/' + align\n",
        "\n",
        "    test_datasets.append(get_dataset(ont1_path, ont2_path, alignment_path))\n",
        "\n",
        "test = pd.concat(test_datasets, ignore_index=True)\n",
        "\n",
        "train.to_csv('/content/dataset_train.csv', index=False)\n",
        "test.to_csv('/content/dataset_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qls3toVX2QPj"
      },
      "source": [
        "# Calculate Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itRumr9GYiGB",
        "outputId": "98fa81cd-1c73-4e99-c9db-aa9b73f841a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/GoogleNews-vectors-negative300.bin.zip\n",
            "  inflating: GoogleNews-vectors-negative300.bin  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/GoogleNews-vectors-negative300.bin.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xlw86_oLZcS8"
      },
      "outputs": [],
      "source": [
        "def camel_case_split(identifier):\n",
        "    matches = finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)',\n",
        "                       identifier)\n",
        "    return [m.group(0) for m in matches]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_wxP5w_yZgz7"
      },
      "outputs": [],
      "source": [
        "def get_word2vec_sim(row_set1, row_set2):\n",
        "    sum_sim = 0\n",
        "    N = max(len(row_set1), len(row_set2))\n",
        "\n",
        "    for w1 in row_set1:\n",
        "        maxSim = 0\n",
        "        for w2 in row_set2:\n",
        "            try:\n",
        "                sim = model.wv.similarity(w1, w2)\n",
        "            except:\n",
        "                sim = 0\n",
        "\n",
        "            if sim > maxSim:\n",
        "                maxSim = sim\n",
        "        sum_sim = sum_sim + maxSim\n",
        "\n",
        "    sum_sim = sum_sim / N\n",
        "\n",
        "    return sum_sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wejItFLlZmgL"
      },
      "outputs": [],
      "source": [
        "def get_words(text):\n",
        "    if '_' in text:\n",
        "        row_set = text.split('_')\n",
        "    else:\n",
        "        if '-' in text:\n",
        "            row_set = text.split('-')\n",
        "        else:\n",
        "            row_set = camel_case_split(text)\n",
        "\n",
        "    row_set = [x.lower() for x in row_set]\n",
        "    return row_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "bDDKP64zZxsO"
      },
      "outputs": [],
      "source": [
        "def calculate_features(dataset, string_type):\n",
        "    ngrams1 = []\n",
        "    ngrams2 = []\n",
        "    ngrams3 = []\n",
        "    ngrams4 = []\n",
        "    dices = []\n",
        "    jaccards = []\n",
        "    jaros = []\n",
        "    mes = []\n",
        "    sws = []\n",
        "    afs = []\n",
        "    coses = []\n",
        "    sfs = []\n",
        "    jws = []\n",
        "    lws = []\n",
        "    tfidfs = []\n",
        "    ovs = []\n",
        "    nws = []\n",
        "    wordnet_sims = []\n",
        "    w2vec_sims = []\n",
        "\n",
        "    if string_type == 'Entity':\n",
        "        index = 2\n",
        "    elif string_type == 'Parent':\n",
        "        index = 4\n",
        "    elif string_type == 'Path':\n",
        "        index = 6\n",
        "\n",
        "    for key, row in tqdm(dataset.iterrows()):\n",
        "\n",
        "        string1 = row[index]\n",
        "        string2 = row[index + 1]\n",
        "\n",
        "        ngrams1.append(ngram.NGram.compare(string1, string2, N=1))\n",
        "        ngrams2.append(ngram.NGram.compare(string1, string2, N=2))\n",
        "        ngrams3.append(ngram.NGram.compare(string1, string2, N=3))\n",
        "        ngrams4.append(ngram.NGram.compare(string1, string2, N=4))\n",
        "        lws.append(lev.get_sim_score(string1, string2))\n",
        "        jaros.append(jaro.get_sim_score(string1, string2))\n",
        "        nws.append(nw.get_raw_score(string1, string2))\n",
        "        sws.append(sw.get_raw_score(string1, string2))\n",
        "        afs.append(af.get_raw_score(string1, string2))\n",
        "        jws.append(jw.get_sim_score(string1, string2))\n",
        "\n",
        "        row_set1 = get_words(string1)\n",
        "        row_set2 = get_words(string2)\n",
        "\n",
        "        mes.append(me.get_raw_score(row_set1, row_set2))\n",
        "        coses.append(cos.get_sim_score(row_set1, row_set2))\n",
        "        sfs.append(sf.get_raw_score(row_set1, row_set2))\n",
        "        tfidfs.append(tfidf.get_sim_score(row_set1, row_set2))\n",
        "        ovs.append(over_coef.get_sim_score(row_set1, row_set2))\n",
        "        dices.append(dice.get_sim_score(row_set1, row_set2))\n",
        "        jaccards.append(jac.get_sim_score(row_set1, row_set2))\n",
        "\n",
        "        allsyns1 = set(ss for word in row_set1 for ss in wordnet.synsets(word))\n",
        "        allsyns2 = set(ss for word in row_set2 for ss in wordnet.synsets(word))\n",
        "\n",
        "        best = [wordnet.wup_similarity(s1, s2) for s1, s2 in itertools.product(allsyns1, allsyns2)]\n",
        "        if len(best) > 0:\n",
        "            wordnet_sims.append(best[0])\n",
        "        else:\n",
        "            wordnet_sims.append(0)\n",
        "\n",
        "        w2vec_sims.append(get_word2vec_sim(row_set1, row_set2))\n",
        "\n",
        "    dataset['Ngram1' + '_' + string_type] = ngrams1\n",
        "    dataset['Ngram2' + '_' + string_type] = ngrams2\n",
        "    dataset['Ngram3' + '_' + string_type] = ngrams3\n",
        "    dataset['Ngram4' + '_' + string_type] = ngrams4\n",
        "    dataset['Dice' + '_' + string_type] = dices\n",
        "    dataset['Jaccard' + '_' + string_type] = jaccards\n",
        "    dataset['Jaro' + '_' + string_type] = jaros\n",
        "    dataset['Monge-Elkan' + '_' + string_type] = mes\n",
        "    dataset['SmithWaterman' + '_' + string_type] = sws\n",
        "    dataset['AffineGap' + '_' + string_type] = afs\n",
        "    dataset['Cosine_similarity' + '_' + string_type] = coses\n",
        "    dataset['Soft_TFIDF' + '_' + string_type] = sfs\n",
        "    dataset['JaroWinkler' + '_' + string_type] = jws\n",
        "    dataset['Levenshtein' + '_' + string_type] = lws\n",
        "    dataset['TFIDF' + '_' + string_type] = tfidfs\n",
        "    dataset['OverlapCoef' + '_' + string_type] = ovs\n",
        "    dataset['Needleman-Wunsch' + '_' + string_type] = nws\n",
        "    dataset['Wordnet_sim' + '_' + string_type] = wordnet_sims\n",
        "    dataset['Word2vec_sim' + '_' + string_type] = w2vec_sims\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Aph25Vha1pqx"
      },
      "outputs": [],
      "source": [
        "af = Affine()\n",
        "me = MongeElkan()\n",
        "nw = NeedlemanWunsch()\n",
        "sw = SmithWaterman()\n",
        "cos = Cosine()\n",
        "sf = SoftTfIdf()\n",
        "jw = JaroWinkler()\n",
        "lev = Levenshtein()\n",
        "dice = Dice()\n",
        "jac = Jaccard()\n",
        "jaro = Jaro()\n",
        "tfidf = TfIdf()\n",
        "over_coef = OverlapCoefficient()\n",
        "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCohvXsb17eD",
        "outputId": "4eeaaaf3-9cfc-42a3-9e77-2d5800e79884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "23040it [04:37, 83.13it/s] \n",
            "23040it [00:26, 864.55it/s] \n",
            "23040it [00:35, 650.53it/s]\n",
            "43704it [08:47, 82.85it/s] \n",
            "43704it [00:50, 858.75it/s] \n",
            "43704it [01:14, 586.56it/s]\n"
          ]
        }
      ],
      "source": [
        "# Calculate features for training dataset\n",
        "data = pd.read_csv('/content/dataset_train.csv')\n",
        "\n",
        "data = calculate_features(data, 'Entity')\n",
        "data = calculate_features(data, 'Parent')\n",
        "data = calculate_features(data, 'Path')\n",
        "\n",
        "data.to_csv('/content/dataset_train_features.csv', index=False)\n",
        "\n",
        "# Calculate features for testing dataset\n",
        "data = pd.read_csv('/content/dataset_test.csv')\n",
        "\n",
        "data = calculate_features(data, 'Entity')\n",
        "data = calculate_features(data, 'Parent')\n",
        "data = calculate_features(data, 'Path')\n",
        "\n",
        "data.to_csv('/content/dataset_test_features.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvgwxSYR2FRH"
      },
      "source": [
        "# Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "M9lJFGyYjKA0"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def train_lr_model(X_train, y_train, X_test, y_test):\n",
        "    model = LogisticRegression(penalty='l2', C=7.742637, class_weight=None)\n",
        "\n",
        "    # Train model\n",
        "    model.fit(X_train, y_train)\n",
        "    y_prob = model.predict_proba(X_test)\n",
        "\n",
        "    return y_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jvLrN3S8kFUe"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def train_rf_model(X_train, y_train, X_test,y_test):\n",
        "    model = RandomForestClassifier(n_estimators=400, max_features='sqrt', max_depth=5, random_state=42)\n",
        "\n",
        "    # Train model\n",
        "    model.fit(X_train, y_train)\n",
        "    y_prob = model.predict_proba(X_test)\n",
        "\n",
        "    return y_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "w4JrrjM9mWnn"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "def train_xgb_model(Xy_train, Xy_test):\n",
        "    param = {'silent': 0, 'objective': 'binary:logistic',\n",
        "                'min_child_weight': 10, 'gamma': 2.0, 'subsample': 0.8,\n",
        "                'colsample_bytree': 0.8, 'max_depth': 10, 'nthread': 6,\n",
        "                'eval_metric': 'error'}\n",
        "    evallist = [(Xy_test, 'eval'), (Xy_train, 'train')]\n",
        "    num_round = 20\n",
        "    \n",
        "    # Train model\n",
        "    bst = xgb.train(param, Xy_train, num_round, evallist,\n",
        "                    verbose_eval=False)\n",
        "\n",
        "    y_prob = bst.predict(Xy_test)\n",
        "\n",
        "    return y_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OIriBce1fbH",
        "outputId": "c34f47f8-8401-424d-c72f-54a7d54fffea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best F1-Score for 101-225.rdf is 1.0 with threshold 0.6\n",
            "Best F1-Score for 101-230.rdf is 0.9868421052631579 with threshold 0.6\n",
            "Best F1-Score for 101-204.rdf is 0.88 with threshold 0.2\n",
            "Best F1-Score for 101-205.rdf is 0.3140495867768595 with threshold 0.7\n",
            "Best F1-Score for 101-301.rdf is 0.1142857142857143 with threshold 0.1\n",
            "Best F1-Score for 101-302.rdf is 0.717948717948718 with threshold 0.1\n",
            "Best F1-Score for 101-303.rdf is 0.782608695652174 with threshold 0.1\n",
            "Best F1-Score for 101-304.rdf is 0.8965517241379309 with threshold 0.5\n"
          ]
        }
      ],
      "source": [
        "selected_model = 'LogisticRegression'\n",
        "\n",
        "train_features = pd.read_csv('/content/dataset_train_features.csv')\n",
        "test_features = pd.read_csv('/content/dataset_test_features.csv')\n",
        "\n",
        "# Create feature \"Type\" for training dataset\n",
        "train_types = []\n",
        "\n",
        "for row in train_features['Type']:\n",
        "    if row == 'Class':\n",
        "        train_types.append(1)\n",
        "    else:\n",
        "        train_types.append(0)\n",
        "\n",
        "train_features['Type_encode'] = train_types\n",
        "\n",
        "# Create feature \"Type\" for testing dataset\n",
        "test_types = []\n",
        "\n",
        "for row in test_features['Type']:\n",
        "    if row == 'Class':\n",
        "        test_types.append(1)\n",
        "    else:\n",
        "        test_types.append(0)\n",
        "\n",
        "test_features['Type_encode'] = test_types\n",
        "\n",
        "X_train = train_features.loc[:, 'Ngram1_Entity':'Type_encode']\n",
        "y_train = train_features['Match']\n",
        "\n",
        "X_test = test_features.loc[:, 'Ngram1_Entity':'Type_encode']\n",
        "y_test = test_features['Match']\n",
        "\n",
        "df_train = train_features.loc[:, 'Ngram1_Entity':'Type_encode']\n",
        "df_train['Match'] = train_features['Match']\n",
        "\n",
        "df_test = test_features.loc[:, 'Ngram1_Entity':'Type_encode']\n",
        "df_test['Match'] = test_features['Match']\n",
        "\n",
        "# Fill nan values with zero\n",
        "X_train = X_train.fillna(value=0)\n",
        "X_test = X_test.fillna(value=0)\n",
        "\n",
        "train = pd.read_csv('/content/dataset_train.csv')\n",
        "test = pd.read_csv('/content/dataset_test.csv')\n",
        "\n",
        "if selected_model != 'XGBoost':\n",
        "    if selected_model == 'LogisticRegression':\n",
        "        y_prob = train_lr_model(X_train, y_train, X_test, y_test)\n",
        "    elif selected_model == 'RandomForest':\n",
        "        y_prob = train_rf_model(X_train, y_train, X_test, y_test)\n",
        "else:\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "    y_prob = train_xgb_model(dtrain, dtest)\n",
        "\n",
        "# Choose best threshold\n",
        "for align in test_aligns:\n",
        "    ont1, ont2 = align.split('.')[0].split('-')\n",
        "    best_ts = 0\n",
        "    best_score = 0\n",
        "\n",
        "    for ts in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
        "\n",
        "        preds = []\n",
        "\n",
        "        if selected_model != 'XGBoost':\n",
        "            for x in y_prob:\n",
        "                if x[1] >= ts:\n",
        "                    preds.append(1)\n",
        "                else:\n",
        "                    preds.append(0)\n",
        "        else:\n",
        "            for x in y_prob:\n",
        "                if x >= ts:\n",
        "                    preds.append(1)\n",
        "                else:\n",
        "                    preds.append(0)\n",
        "\n",
        "        test['Predict'] = preds\n",
        "\n",
        "        pred_mappings = test[(test['Ontology1'] == f\"/content/drive/MyDrive/Dataset/ontologies/{ont1}.rdf\") & (test['Ontology2'] == f\"/content/drive/MyDrive/Dataset/ontologies/{ont2}.rdf\") & (test['Predict'] == 1)]\n",
        "\n",
        "        true_mappings = test[(test['Ontology1'] == f\"/content/drive/MyDrive/Dataset/ontologies/{ont1}.rdf\") & (test['Ontology2'] == f\"/content/drive/MyDrive/Dataset/ontologies/{ont2}.rdf\") & (test['Match'] == 1)]\n",
        "\n",
        "        correct_mappings = test[(test['Ontology1'] == f\"/content/drive/MyDrive/Dataset/ontologies/{ont1}.rdf\") & (test['Ontology2'] == f\"/content/drive/MyDrive/Dataset/ontologies/{ont2}.rdf\") & (test['Match'] == 1) & (test['Predict'] == 1)]\n",
        "\n",
        "        true_count = len(true_mappings)\n",
        "        pred_count = len(pred_mappings)\n",
        "        corr_count = len(correct_mappings)\n",
        "\n",
        "        if pred_count != 0 and true_count != 0 and corr_count != 0:\n",
        "            precision = corr_count / pred_count\n",
        "            recall = corr_count / true_count\n",
        "            score = 2 * precision * recall / (precision + recall)\n",
        "        else:\n",
        "            score = 0\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_ts = ts\n",
        "            best_preds = preds\n",
        "\n",
        "    print(\n",
        "        f\"Best F1-Score for {align} is {best_score} with threshold {best_ts}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_model = 'RandomForest'\n",
        "\n",
        "train_features = pd.read_csv('/content/dataset_train_features.csv')\n",
        "test_features = pd.read_csv('/content/dataset_test_features.csv')\n",
        "\n",
        "# Create feature \"Type\" for training dataset\n",
        "train_types = []\n",
        "\n",
        "for row in train_features['Type']:\n",
        "    if row == 'Class':\n",
        "        train_types.append(1)\n",
        "    else:\n",
        "        train_types.append(0)\n",
        "\n",
        "train_features['Type_encode'] = train_types\n",
        "\n",
        "# Create feature \"Type\" for testing dataset\n",
        "test_types = []\n",
        "\n",
        "for row in test_features['Type']:\n",
        "    if row == 'Class':\n",
        "        test_types.append(1)\n",
        "    else:\n",
        "        test_types.append(0)\n",
        "\n",
        "test_features['Type_encode'] = test_types\n",
        "\n",
        "X_train = train_features.loc[:, 'Ngram1_Entity':'Type_encode']\n",
        "y_train = train_features['Match']\n",
        "\n",
        "X_test = test_features.loc[:, 'Ngram1_Entity':'Type_encode']\n",
        "y_test = test_features['Match']\n",
        "\n",
        "df_train = train_features.loc[:, 'Ngram1_Entity':'Type_encode']\n",
        "df_train['Match'] = train_features['Match']\n",
        "\n",
        "df_test = test_features.loc[:, 'Ngram1_Entity':'Type_encode']\n",
        "df_test['Match'] = test_features['Match']\n",
        "\n",
        "# Fill nan values with zero\n",
        "X_train = X_train.fillna(value=0)\n",
        "X_test = X_test.fillna(value=0)\n",
        "\n",
        "train = pd.read_csv('/content/dataset_train.csv')\n",
        "test = pd.read_csv('/content/dataset_test.csv')\n",
        "\n",
        "if selected_model != 'XGBoost':\n",
        "    if selected_model == 'LogisticRegression':\n",
        "        y_prob = train_lr_model(X_train, y_train, X_test, y_test)\n",
        "    elif selected_model == 'RandomForest':\n",
        "        y_prob = train_rf_model(X_train, y_train, X_test, y_test)\n",
        "else:\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "    y_prob = train_xgb_model(dtrain, dtest)\n",
        "\n",
        "# Choose best threshold\n",
        "for align in test_aligns:\n",
        "    ont1, ont2 = align.split('.')[0].split('-')\n",
        "    best_ts = 0\n",
        "    best_score = 0\n",
        "\n",
        "    for ts in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
        "\n",
        "        preds = []\n",
        "\n",
        "        if selected_model != 'XGBoost':\n",
        "            for x in y_prob:\n",
        "                if x[1] >= ts:\n",
        "                    preds.append(1)\n",
        "                else:\n",
        "                    preds.append(0)\n",
        "        else:\n",
        "            for x in y_prob:\n",
        "                if x >= ts:\n",
        "                    preds.append(1)\n",
        "                else:\n",
        "                    preds.append(0)\n",
        "\n",
        "        test['Predict'] = preds\n",
        "\n",
        "        pred_mappings = test[(test['Ontology1'] == f\"/content/drive/MyDrive/Dataset/ontologies/{ont1}.rdf\") & (test['Ontology2'] == f\"/content/drive/MyDrive/Dataset/ontologies/{ont2}.rdf\") & (test['Predict'] == 1)]\n",
        "\n",
        "        true_mappings = test[(test['Ontology1'] == f\"/content/drive/MyDrive/Dataset/ontologies/{ont1}.rdf\") & (test['Ontology2'] == f\"/content/drive/MyDrive/Dataset/ontologies/{ont2}.rdf\") & (test['Match'] == 1)]\n",
        "\n",
        "        correct_mappings = test[(test['Ontology1'] == f\"/content/drive/MyDrive/Dataset/ontologies/{ont1}.rdf\") & (test['Ontology2'] == f\"/content/drive/MyDrive/Dataset/ontologies/{ont2}.rdf\") & (test['Match'] == 1) & (test['Predict'] == 1)]\n",
        "\n",
        "        true_count = len(true_mappings)\n",
        "        pred_count = len(pred_mappings)\n",
        "        corr_count = len(correct_mappings)\n",
        "\n",
        "        if pred_count != 0 and true_count != 0 and corr_count != 0:\n",
        "            precision = corr_count / pred_count\n",
        "            recall = corr_count / true_count\n",
        "            score = 2 * precision * recall / (precision + recall)\n",
        "        else:\n",
        "            score = 0\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_ts = ts\n",
        "            best_preds = preds\n",
        "\n",
        "    print(\n",
        "        f\"Best F1-Score for {align} is {best_score} with threshold {best_ts}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVAyhNN6GSh8",
        "outputId": "019b9625-7b34-4a72-8380-b67685fe88a1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best F1-Score for 101-225.rdf is 1.0 with threshold 0.5\n",
            "Best F1-Score for 101-230.rdf is 0.9868421052631579 with threshold 0.5\n",
            "Best F1-Score for 101-204.rdf is 0.888888888888889 with threshold 0.2\n",
            "Best F1-Score for 101-205.rdf is 0.2900763358778626 with threshold 0.2\n",
            "Best F1-Score for 101-301.rdf is 0.26666666666666666 with threshold 0.5\n",
            "Best F1-Score for 101-302.rdf is 0.717948717948718 with threshold 0.1\n",
            "Best F1-Score for 101-303.rdf is 0.7956989247311828 with threshold 0.1\n",
            "Best F1-Score for 101-304.rdf is 0.9324324324324325 with threshold 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_model = 'XGBoost'\n",
        "\n",
        "train_features = pd.read_csv('/content/dataset_train_features.csv')\n",
        "test_features = pd.read_csv('/content/dataset_test_features.csv')\n",
        "\n",
        "# Create feature \"Type\" for training dataset\n",
        "train_types = []\n",
        "\n",
        "for row in train_features['Type']:\n",
        "    if row == 'Class':\n",
        "        train_types.append(1)\n",
        "    else:\n",
        "        train_types.append(0)\n",
        "\n",
        "train_features['Type_encode'] = train_types\n",
        "\n",
        "# Create feature \"Type\" for testing dataset\n",
        "test_types = []\n",
        "\n",
        "for row in test_features['Type']:\n",
        "    if row == 'Class':\n",
        "        test_types.append(1)\n",
        "    else:\n",
        "        test_types.append(0)\n",
        "\n",
        "test_features['Type_encode'] = test_types\n",
        "\n",
        "X_train = train_features.loc[:, 'Ngram1_Entity':'Type_encode']\n",
        "y_train = train_features['Match']\n",
        "\n",
        "X_test = test_features.loc[:, 'Ngram1_Entity':'Type_encode']\n",
        "y_test = test_features['Match']\n",
        "\n",
        "df_train = train_features.loc[:, 'Ngram1_Entity':'Type_encode']\n",
        "df_train['Match'] = train_features['Match']\n",
        "\n",
        "df_test = test_features.loc[:, 'Ngram1_Entity':'Type_encode']\n",
        "df_test['Match'] = test_features['Match']\n",
        "\n",
        "# Fill nan values with zero\n",
        "X_train = X_train.fillna(value=0)\n",
        "X_test = X_test.fillna(value=0)\n",
        "\n",
        "train = pd.read_csv('/content/dataset_train.csv')\n",
        "test = pd.read_csv('/content/dataset_test.csv')\n",
        "\n",
        "if selected_model != 'XGBoost':\n",
        "    if selected_model == 'LogisticRegression':\n",
        "        y_prob = train_lr_model(X_train, y_train, X_test, y_test)\n",
        "    elif selected_model == 'RandomForest':\n",
        "        y_prob = train_rf_model(X_train, y_train, X_test, y_test)\n",
        "else:\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "    y_prob = train_xgb_model(dtrain, dtest)\n",
        "\n",
        "# Choose best threshold\n",
        "for align in test_aligns:\n",
        "    ont1, ont2 = align.split('.')[0].split('-')\n",
        "    best_ts = 0\n",
        "    best_score = 0\n",
        "\n",
        "    for ts in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
        "\n",
        "        preds = []\n",
        "\n",
        "        if selected_model != 'XGBoost':\n",
        "            for x in y_prob:\n",
        "                if x[1] >= ts:\n",
        "                    preds.append(1)\n",
        "                else:\n",
        "                    preds.append(0)\n",
        "        else:\n",
        "            for x in y_prob:\n",
        "                if x >= ts:\n",
        "                    preds.append(1)\n",
        "                else:\n",
        "                    preds.append(0)\n",
        "\n",
        "        test['Predict'] = preds\n",
        "\n",
        "        pred_mappings = test[(test['Ontology1'] == f\"/content/drive/MyDrive/Dataset/ontologies/{ont1}.rdf\") & (test['Ontology2'] == f\"/content/drive/MyDrive/Dataset/ontologies/{ont2}.rdf\") & (test['Predict'] == 1)]\n",
        "\n",
        "        true_mappings = test[(test['Ontology1'] == f\"/content/drive/MyDrive/Dataset/ontologies/{ont1}.rdf\") & (test['Ontology2'] == f\"/content/drive/MyDrive/Dataset/ontologies/{ont2}.rdf\") & (test['Match'] == 1)]\n",
        "\n",
        "        correct_mappings = test[(test['Ontology1'] == f\"/content/drive/MyDrive/Dataset/ontologies/{ont1}.rdf\") & (test['Ontology2'] == f\"/content/drive/MyDrive/Dataset/ontologies/{ont2}.rdf\") & (test['Match'] == 1) & (test['Predict'] == 1)]\n",
        "\n",
        "        true_count = len(true_mappings)\n",
        "        pred_count = len(pred_mappings)\n",
        "        corr_count = len(correct_mappings)\n",
        "\n",
        "        if pred_count != 0 and true_count != 0 and corr_count != 0:\n",
        "            precision = corr_count / pred_count\n",
        "            recall = corr_count / true_count\n",
        "            score = 2 * precision * recall / (precision + recall)\n",
        "        else:\n",
        "            score = 0\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_ts = ts\n",
        "            best_preds = preds\n",
        "\n",
        "    print(\n",
        "        f\"Best F1-Score for {align} is {best_score} with threshold {best_ts}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rUwRBOGGWJv",
        "outputId": "f9a820b2-799c-48d0-8c5c-2db5f16729e4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/xgboost/core.py:617: FutureWarning: Pass `evals` as keyword args.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[16:43:53] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"silent\" } are not used.\n",
            "\n",
            "Best F1-Score for 101-225.rdf is 0.9626168224299065 with threshold 0.6\n",
            "Best F1-Score for 101-230.rdf is 0.9554140127388536 with threshold 0.7\n",
            "Best F1-Score for 101-204.rdf is 0.752808988764045 with threshold 0.6\n",
            "Best F1-Score for 101-205.rdf is 0.256 with threshold 0.7\n",
            "Best F1-Score for 101-301.rdf is 0.273972602739726 with threshold 0.7\n",
            "Best F1-Score for 101-302.rdf is 0.717948717948718 with threshold 0.1\n",
            "Best F1-Score for 101-303.rdf is 0.7956989247311828 with threshold 0.1\n",
            "Best F1-Score for 101-304.rdf is 0.9139072847682119 with threshold 0.6\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}